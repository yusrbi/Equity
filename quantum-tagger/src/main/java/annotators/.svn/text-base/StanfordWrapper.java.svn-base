package annotators;

import java.io.IOException;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Properties;

import loader.DocumentLoader;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.base.Joiner;

import data.Document_;
import data.Table_;
import edu.stanford.nlp.ie.AbstractSequenceClassifier;
import edu.stanford.nlp.ie.crf.CRFClassifier;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeCoreAnnotations;
import edu.stanford.nlp.trees.TreeCoreAnnotations.TreeAnnotation;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.Triple;
import edu.stanford.nlp.simple.*;


public class StanfordWrapper implements AnnotatorWrapper {

	private static StanfordWrapper stanford_wrapper;
	private static StanfordCoreNLP pipeline = null;
	private final static String serializedClassifier = "classifiers/english.muc.7class.distsim.crf.ser.gz";
	private CRFClassifier classifier;
	private static Logger slogger_ = LoggerFactory
			.getLogger(DocumentLoader.class);

	private StanfordWrapper() throws ClassCastException,
			ClassNotFoundException, IOException {
		 Properties props = new Properties();
		 props.setProperty("annotators", "tokenize,ssplit,pos, parse");
		 props.setProperty("ner.model", serializedClassifier);
		pipeline = new StanfordCoreNLP(props);
		classifier = CRFClassifier.getClassifier(serializedClassifier);
	}

	public static StanfordWrapper getStanfordWrapper()
			throws ClassCastException, ClassNotFoundException, IOException {
		if (stanford_wrapper == null)
			stanford_wrapper = new StanfordWrapper();
		return stanford_wrapper;
	}

//	/**
//	 * process the document and its tables as a single document
//	 * @param document
//	 */
//	public void processAllOnce(Document_ document) {// using the simple is faster, but
//		
//		slogger_.info("processing document: " + document.getId());
//		
//		List<Triple<String, Integer, Integer>> triples = classifier
//				.classifyToCharacterOffsets(document.getContentAndTables());
//		document.processNEAnnotations(triples);
//		
//		Annotation doc_annotations = new Annotation(document.getContentAndTables());
//		pipeline.annotate(doc_annotations);		
//		List<CoreMap> sentences = doc_annotations.get(SentencesAnnotation.class);
//		document.processPOSAnnotations(sentences);
//	}

	public List<String> getNounPhrases(String text){
		Annotation document = new Annotation(text);
		// run all Annotators on this text
		pipeline.annotate(document);
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		List<String> np = new LinkedList<String>();
		for (CoreMap sentence : sentences) {
			 Tree tree = sentence.get(TreeAnnotation.class);
			  dfs(tree.firstChild(),np);
		}
		return np;
	}
	private void dfs(Tree tree, List<String> np) {
		if(tree == null)
			return;
		if(tree.label().value().equals("NP")){
			StringBuilder text = new StringBuilder();
			for(Tree leave: tree.getLeaves()){
				text.append(leave.value()+" ");
			}
			np.add(text.toString().trim().replace("-LRB-", "(").replace("-RRB-", ")").replace("''", ""));			
		}
		for(Tree child : tree.children()){
			dfs(child,np);
		}
	}

	public void process(String text) {
		// Properties props = new Properties();
		// props.setProperty("annotators", "tokenize, ssplit, pos, ner");
		// StanfordCoreNLP pipeline = new StanfordCoreNLP(props);

		// create an empty Annotation just with the given text
		Annotation document = new Annotation(text);
		// run all Annotators on this text
		pipeline.annotate(document);

		// these are all the sentences in this document
		// a CoreMap is essentially a Map that uses class objects as keys and
		// has values with custom types
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		
		for (CoreMap sentence : sentences) {
			
			// traversing the words in the current sentence
			// a CoreLabel is a CoreMap with additional token-specific methods
			for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
				// this is the text of the token
				String word = token.get(TextAnnotation.class);
				// this is the POS tag of the token
				String pos = token.get(PartOfSpeechAnnotation.class);
				// this is the NER label of the token
				String ne = token.get(NamedEntityTagAnnotation.class);
				token.endPosition();
				token.beginPosition();
			}
			// this is the Stanford dependency graph of the current sentence
			// SemanticGraph dependencies =
			// sentence.get(CollapsedCCProcessedDependenciesAnnotation.class);
		}

		// This is the coreference link graph
		// Each chain stores a set of mentions that link to each other,
		// along with a method for getting the most representative mention
		// Both sentence and token offsets start at 1!
		// Map<Integer, CorefChain> graph =
		// document.get(CorefChainAnnotation.class);

	}

	public void process(Document_ document) {// using the simple is faster, but
												// if we will load the core NLP
												// each time then it easier to
												// load once.
		// edu.stanford.nlp.simple.Document page_doc = new
		// edu.stanford.nlp.simple.Document(document.getContent());
		slogger_.info("processing document: " + document.getId());
		// int temp=0;
		if(document.getContent() != null){
			List<Triple<String, Integer, Integer>> triples = classifier
					.classifyToCharacterOffsets(document.getContent());
			document.processAnnotations(triples);
		}
		
		
		for (Table_ table : document.getTables()) {

			//triples = classifier
				//	.classifyToCharacterOffsets(table.getAsString());
			String result = classifier.classifyToString(table.getAsString(),
					"tsv", false);
			table.processAnnotaions(result);
			//table.processAnnotaions(triples);
		}
		// this is not needed any more as we taking all noun phrases in the text
		//document.propagateAnnotationsToTables(); // stop the propagation 
		
		
				
		// StringBuilder annotations = new StringBuilder();
		//
		// for (Triple<String, Integer, Integer> trip : triples) {
		// annotations.append(String.format("%s[%d, %d)\n",trip.first(),
		// trip.second(), trip.third()));
		// }
		// document.setAnnotations(annotations.toString());

		// for(Sentence sent : page_doc.sentences() ){
		// sent.nerTags();
		// }
		// edu.stanford.nlp.simple.Document table_doc;
		// for(Table_ table : document.getTables()){
		// table_doc = new edu.stanford.nlp.simple.Document(table.getContent());
		// //String xml_table =
		// table_doc.xml(edu.stanford.nlp.simple.Document::ner); does not work
		// ner method is not impelmented
		// for (Sentence sent : table_doc.sentences()){
		// sent.nerTags();
		// }
		//
		// }

	}

	
}

//"annotators"    <== "tokenize, ssplit, pos, lemma, ner, parse, dcoref"
//"pos.model"     <== ! @"pos-tagger\english-bidirectional\english-bidirectional-distsim.tagger"
//"ner.model"     <== ! @"ner\english.all.3class.distsim.crf.ser.gz"
//"parse.model"   <== ! @"lexparser\englishPCFG.ser.gz"
// 
//"dcoref.demonym"            <== ! @"dcoref\demonyms.txt"
//"dcoref.states"             <== ! @"dcoref\state-abbreviations.txt"
//"dcoref.animate"            <== ! @"dcoref\animate.unigrams.txt"
//"dcoref.inanimate"          <== ! @"dcoref\inanimate.unigrams.txt"
//"dcoref.male"               <== ! @"dcoref\male.unigrams.txt"
//"dcoref.neutral"            <== ! @"dcoref\neutral.unigrams.txt"
//"dcoref.female"             <== ! @"dcoref\female.unigrams.txt"
//"dcoref.plural"             <== ! @"dcoref\plural.unigrams.txt"
//"dcoref.singular"           <== ! @"dcoref\singular.unigrams.txt"
//"dcoref.countries"          <== ! @"dcoref\countries"
//"dcoref.extra.gender"       <== ! @"dcoref\namegender.combine.txt"
//"dcoref.states.provinces"   <== ! @"dcoref\statesandprovinces"
//"dcoref.singleton.predictor"<== ! @"dcoref\singleton.predictor.ser
